{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vada_pav.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eshaanrathi2/vada-pav-classifier/blob/master/vada_pav.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSBJP7dJQUZI",
        "colab_type": "text"
      },
      "source": [
        "**First I created a dataset of vada_pav and not_vada_pav images via Google. Used Chrome store application for multiple images download. Cleaned the downloaded data and created Train and Test image folders having vada_pav and not_vada_pav each. I have uploaded these folders on my Google drive. Here's the link : \n",
        "https://drive.google.com/drive/folders/100tmP-8bvE2Fe99d0ihQEGqX1JpZkIFT?usp=sharing\n",
        "For not_vada_pav I used Indian food other than vada_pav, humans, places, things, random_objects etc.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgg1jgs_PaUu",
        "colab_type": "text"
      },
      "source": [
        "**Mounting my Google drive with the Colab notebook. As the dataset is on my Google drive and would be saving weights over there.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEQdaekjzVac",
        "colab_type": "code",
        "outputId": "30accede-04ec-4e65-c8da-eb493eca5ea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXcqEqmZPvOY",
        "colab_type": "text"
      },
      "source": [
        "**Importing few dependencies/libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLzEb9DiMKwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import keras\n",
        "from keras.layers import Dense, Activation, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from keras.applications import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input #importing preprocessing unit for inceptionv3 has a different syntax\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.models import Model, Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint # to save our model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7c-xMmF09vW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqu439EScuke",
        "colab_type": "text"
      },
      "source": [
        "**Creating batch of train images :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LDXWxG3Tvd1",
        "colab_type": "code",
        "outputId": "36b76360-bf17-4d94-e43c-1bfbd599f1f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_generator = datagen.flow_from_directory('/content/gdrive/My Drive/vada_pav/dataset/train', # this is where you specify the path to the main data folder\n",
        "                                                 target_size = (224,224),\n",
        "                                                 color_mode = 'rgb',\n",
        "                                                 batch_size = 32,\n",
        "                                                 shuffle = True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 658 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOcb_sysP6rv",
        "colab_type": "text"
      },
      "source": [
        "**Creating batch of test images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SEf7wy5UBdP",
        "colab_type": "code",
        "outputId": "b5b57814-eb7f-4e05-ffce-b21af9b8c2a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_generator = datagen.flow_from_directory('/content/gdrive/My Drive/vada_pav/dataset/test', # this is where you specify the path to the main data folder\n",
        "                                                 target_size = (224,224),\n",
        "                                                 color_mode = 'rgb',\n",
        "                                                 batch_size = 32,\n",
        "                                                 shuffle = True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 159 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_ovhAv2dDTf",
        "colab_type": "text"
      },
      "source": [
        "**Architechture Construction :\n",
        "Using InceptionV3 as the base for the model. Then added FC and Dense layer to it, with 2 output classes i.e vada_pav and not_vada_pav.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoR4MbukeCTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model = InceptionV3(weights = 'imagenet',include_top = False, input_shape = (224,224,3))\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(2, activation = 'softmax')(x)\n",
        "\n",
        "model = Model(base_model.input, x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CWmwsZaRg_r",
        "colab_type": "text"
      },
      "source": [
        "**Now we would be only training last few layers and keep the weights of InceptionV3 intact.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hlpAe8Hk0k4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsz83yVvdkWA",
        "colab_type": "text"
      },
      "source": [
        "**Compiling the model :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qomLwH4PuMIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer = Adam(lr=0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Mj4jSWZdrv-",
        "colab_type": "text"
      },
      "source": [
        "**Checkpointing for better training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2gTZEQIwUpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath = '/content/gdrive/My Drive/vada_pav/weights.hdf5', verbose=1, save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Q0KagStysyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "step_size_train = train_generator.n//train_generator.batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoZ8VSXz1QMQ",
        "colab_type": "text"
      },
      "source": [
        "**Few images are corrupted in my dataset even after cleaning. Hence following lines would solve the issue. Otherwise, a \"image file is truncated\" error will occur.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfLtLgT31I0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DixF50xghwLZ",
        "colab_type": "text"
      },
      "source": [
        "**Now Training the above model** : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h91ALuYTxA6p",
        "colab_type": "code",
        "outputId": "b34fd6ff-0fd1-4016-9d90-0ee483b5a412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        }
      },
      "source": [
        "history = model.fit(train_generator, epochs=15, callbacks=[checkpointer], steps_per_epoch = step_size_train)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            " 2/20 [==>...........................] - ETA: 2:42 - loss: 0.8953 - acc: 0.7656"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:914: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
            "  'to RGBA images')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 268s 13s/step - loss: 1.3877 - acc: 0.7369\n",
            "Epoch 2/15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 6s 309ms/step - loss: 1.0490 - acc: 0.8219\n",
            "Epoch 3/15\n",
            "20/20 [==============================] - 6s 295ms/step - loss: 0.3007 - acc: 0.9315\n",
            "Epoch 4/15\n",
            "20/20 [==============================] - 6s 304ms/step - loss: 0.0880 - acc: 0.9719\n",
            "Epoch 5/15\n",
            "20/20 [==============================] - 6s 288ms/step - loss: 0.0402 - acc: 0.9796\n",
            "Epoch 6/15\n",
            "20/20 [==============================] - 6s 320ms/step - loss: 0.0555 - acc: 0.9797\n",
            "Epoch 7/15\n",
            "20/20 [==============================] - 6s 293ms/step - loss: 0.0435 - acc: 0.9816\n",
            "Epoch 8/15\n",
            "20/20 [==============================] - 5s 273ms/step - loss: 0.1350 - acc: 0.9593\n",
            "Epoch 9/15\n",
            "20/20 [==============================] - 6s 296ms/step - loss: 0.1673 - acc: 0.9492\n",
            "Epoch 10/15\n",
            "20/20 [==============================] - 6s 311ms/step - loss: 0.0413 - acc: 0.9922\n",
            "Epoch 11/15\n",
            "20/20 [==============================] - 6s 301ms/step - loss: 0.0201 - acc: 0.9937\n",
            "Epoch 12/15\n",
            "20/20 [==============================] - 6s 304ms/step - loss: 0.0096 - acc: 0.9969\n",
            "Epoch 13/15\n",
            "20/20 [==============================] - 6s 312ms/step - loss: 0.0360 - acc: 0.9906\n",
            "Epoch 14/15\n",
            "20/20 [==============================] - 6s 291ms/step - loss: 0.0167 - acc: 0.9926\n",
            "Epoch 15/15\n",
            "20/20 [==============================] - 6s 304ms/step - loss: 0.0056 - acc: 0.9969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1OMymsqhy9Y",
        "colab_type": "text"
      },
      "source": [
        "**Evaluation (on unseen data) :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWDpx_AEgPk2",
        "colab_type": "code",
        "outputId": "1df8a37d-a15d-4926-d601-b1aa04200762",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "step_size_test = test_generator.n//test_generator.batch_size\n",
        "model.evaluate_generator(test_generator, verbose=1, steps=step_size_test)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 70s 18s/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7076681479811668, 0.8984375]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnMZUk42iobH",
        "colab_type": "text"
      },
      "source": [
        "**Predictions :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HLON0CaCvNr",
        "colab_type": "code",
        "outputId": "79e6b17f-1a4b-43fb-afd9-a588b50831ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_generator.reset()\n",
        "preds = model.predict_generator(test_generator, steps= step_size_test, verbose=1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 3s 729ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuYe0CNnDRwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_class_indices = np.argmax(preds,axis=1) # These are labels created after predictions\n",
        "\n",
        "labels = (test_generator.class_indices) \n",
        "labels = dict((v,k) for k,v in labels.items())\n",
        "predictions = [labels[k] for k in predicted_class_indices] #These are true labels from test set. Feteching them from my folder's name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN6Oi51xp29d",
        "colab_type": "text"
      },
      "source": [
        "**Saving the model :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t7kfO2f-05j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('/content/gdrive/My Drive/vada_pav/weights.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}